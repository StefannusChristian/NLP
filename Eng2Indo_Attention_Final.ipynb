{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rycMa52tWIy4"
   },
   "source": [
    "## English to Indonesian translation using attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "l2qpBg3uWIy6"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "tuy6ysg_WIy-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Loss function: https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(device)\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "l4LvE_r2WIzB",
    "outputId": "2e8f940e-bc78-4c96-cd6e-5f0ac477df4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\anaconda3\\envs\\uas-nlp-torch\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "kqBsanwyWIzE",
    "outputId": "428053ee-79ad-49a4-8cf5-f09fc5d74dc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6752, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Indonesian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>run !</td>\n",
       "      <td>lari !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>who ?</td>\n",
       "      <td>siapa ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wow !</td>\n",
       "      <td>wow !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>help !</td>\n",
       "      <td>tolong !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jump !</td>\n",
       "      <td>lompat !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English Indonesian\n",
       "0   run !     lari !\n",
       "1   who ?    siapa ?\n",
       "2   wow !      wow !\n",
       "3  help !   tolong !\n",
       "4  jump !   lompat !"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the input file which has the English and the Bahasa sentence pairs separated by tab\n",
    "fp = open('eng-indo.txt', 'r')\n",
    "text = fp.read()\n",
    "text = text.splitlines()\n",
    "fp.close()\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "text_dict = {\"English\": [], \"Indonesian\": []}\n",
    "for l in text:\n",
    "    split_text = l.split(\"\\t\")\n",
    "    text_dict[\"English\"].append(normalizeString(split_text[0]))\n",
    "    text_dict[\"Indonesian\"].append(normalizeString(split_text[1]))\n",
    "\n",
    "df = pd.DataFrame.from_dict(text_dict)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "O4BBhVu5WIzL"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 25\n",
    "MIN_LENGTH = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "man1RTr1WIzO",
    "outputId": "68996a6a-87cb-4cde-f69e-8125ad5565d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6752, 3)\n",
      "Current shape: (6752, 3)\n",
      "New shape: (6609, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>English</th>\n",
       "      <th>Indonesian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>i m sad .</td>\n",
       "      <td>saya sedih .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>it s me !</td>\n",
       "      <td>ini aku !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>i get it .</td>\n",
       "      <td>aku mengerti .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>i got it .</td>\n",
       "      <td>aku mengerti .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>i m okay .</td>\n",
       "      <td>aku baik baik saja .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     English            Indonesian\n",
       "0     34   i m sad .          saya sedih .\n",
       "1     35   it s me !             ini aku !\n",
       "2     53  i get it .        aku mengerti .\n",
       "3     54  i got it .        aku mengerti .\n",
       "4     57  i m okay .  aku baik baik saja ."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def should_keep_row(row):\n",
    "    \"\"\" Should the current row be kept as training set\"\"\"\n",
    "    # indo_num_words = len(word_tokenize(row[\"Indonesian\"]))\n",
    "    eng_num_words = len(word_tokenize(row[\"English\"]))\n",
    "    max_words_required = MAX_LENGTH - 2\n",
    "    min_words_required = MIN_LENGTH\n",
    "\n",
    "    return min_words_required <= eng_num_words <= max_words_required\n",
    "\n",
    "df[\"keep_row\"] = df.apply(should_keep_row, axis=1)\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "print(\"Current shape: \" + str(df.shape))\n",
    "df = df[df[\"keep_row\"]]\n",
    "print(\"New shape: \" + str(df.shape))\n",
    "df.head()\n",
    "df = df.reset_index().drop(columns=[\"keep_row\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "cZZf29huWIzS",
    "outputId": "5fbc5d92-5991-47a4-e901-3e5be81c46aa"
   },
   "outputs": [],
   "source": [
    "# Use a unique string to indicate START and END of a sentence.\n",
    "# Assign a unique index to them.\n",
    "START, START_IDX = '<s>',  0\n",
    "END, END_IDX = '</s>', 1\n",
    "UNK, UNK_IDX = 'UNK', 2\n",
    "\n",
    "SOS_token = START_IDX\n",
    "EOS_token = END_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use this idiom to tokenize our sentences in the dataframe column:\n",
    "# >>> DataFrame['column'].apply(str.lower).apply(word_tokenize)\n",
    "\n",
    "# Also we added the START and the END symbol to the sentences.\n",
    "# Tokenize English sentences and add START and END tokens\n",
    "english_sents = df['English'].apply(str.lower).apply(\n",
    "    word_tokenize).apply(lambda x: [START] + x + [END])\n",
    "\n",
    "# Tokenize Indonesian sentences and add START and END tokens\n",
    "indo_sents = df['Indonesian'].apply(str.lower).apply(\n",
    "    word_tokenize).apply(lambda x: [START] + x + [END])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First English sentence: ['<s>', 'i', 'm', 'sad', '.', '</s>']\n",
      "First Indo sentence: ['<s>', 'saya', 'sedih', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# We're sort of getting into the data into the shape we want.\n",
    "# But now it's still too humanly readable and redundant.\n",
    "## Cut-away: Computers like it to be simpler, more concise. -_-|||\n",
    "print('First English sentence:', english_sents[0])\n",
    "print('First Indo sentence:', indo_sents[0])\n",
    "\n",
    "english_vocab = Dictionary([['<s>'], ['</s>'],['UNK']])\n",
    "english_vocab.add_documents(english_sents)\n",
    "\n",
    "indo_vocab = Dictionary([['<s>'], ['</s>'], ['UNK']])\n",
    "indo_vocab.add_documents(indo_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Indonesian words in Dictionary:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '.'), (4, 'saya'), (5, 'sedih'), (6, '!'), (7, 'aku'), (8, 'ini'), (9, 'mengerti')]\n",
      "\n",
      "First 10 English words in Dictionary:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '.'), (4, 'i'), (5, 'm'), (6, 'sad'), (7, '!'), (8, 'it'), (9, 'me')]\n",
      "First 10 Indonesian words in Dictionary:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '.'), (4, 'saya'), (5, 'sedih'), (6, '!'), (7, 'aku'), (8, 'ini'), (9, 'mengerti')]\n",
      "\n",
      "First 10 English words in Dictionary:\n",
      " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '.'), (4, 'i'), (5, 'm'), (6, 'sad'), (7, '!'), (8, 'it'), (9, 'me')]\n"
     ]
    }
   ],
   "source": [
    "# First ten words in the vocabulary.\n",
    "print('First 10 Indonesian words in Dictionary:\\n', sorted(indo_vocab.items())[:10])\n",
    "print()\n",
    "print('First 10 English words in Dictionary:\\n', sorted(english_vocab.items())[:10])\n",
    "\n",
    "english_vocab = Dictionary([['<s>'], ['</s>'],['UNK']])\n",
    "english_vocab.add_documents(english_sents)\n",
    "\n",
    "indo_vocab = Dictionary([['<s>'], ['</s>'], ['UNK']])\n",
    "indo_vocab.add_documents(indo_sents)\n",
    "\n",
    "# First ten words in the vocabulary.\n",
    "print('First 10 Indonesian words in Dictionary:\\n', sorted(indo_vocab.items())[:10])\n",
    "print()\n",
    "print('First 10 English words in Dictionary:\\n', sorted(english_vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "RjdR62hJWIzW"
   },
   "outputs": [],
   "source": [
    "#input val_sent_pairs[0] english input to translate output is candidate\n",
    "#val_sent_pairs[1] reference\n",
    "def calculate_bleu_score(reference_sent,candidate_sent):\n",
    "    reference = [word_tokenize(reference_sent)]\n",
    "    candidate = word_tokenize(candidate_sent)\n",
    "\n",
    "    if '<s>' in candidate:\n",
    "        candidate.remove('<s>')\n",
    "    if '</s>' in candidate:\n",
    "        candidate.remove('</s>')\n",
    "    gram_1_score = sentence_bleu(reference,candidate,weights=(1, 0, 0, 0))\n",
    "    gram_2_score = sentence_bleu(reference,candidate,weights=(0.5, 0.5, 0, 0))\n",
    "    gram_3_score = sentence_bleu(reference,candidate,weights=(0.33, 0.33, 0.33, 0))\n",
    "    gram_4_score = sentence_bleu(reference,candidate,weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    blue_score = (gram_1_score+gram_2_score+gram_3_score+gram_4_score)/4\n",
    "    #print(blue_score)\n",
    "    return blue_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "aLocWKZ_WIzd",
    "outputId": "2da4f69f-d088-463d-827e-64c6b8a881a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0],\n",
       "        [32],\n",
       "        [ 8],\n",
       "        [45],\n",
       "        [15],\n",
       "        [ 1]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorizes a sentence with a given vocab\n",
    "def vectorize_sent(sent, vocab):\n",
    "    return vocab.doc2idx([START] + word_tokenize(sent.lower()) + [END], unknown_word_index=2)\n",
    "\n",
    "# Creates a PyTorch variable from a sentence against a given vocab\n",
    "def variable_from_sent(sent, vocab):\n",
    "    vsent = vectorize_sent(sent, vocab)\n",
    "    #print(vsent)\n",
    "    result = Variable(torch.LongTensor(vsent).view(-1, 1))\n",
    "    #print(result)\n",
    "    return result.cuda() if use_cuda else result\n",
    "\n",
    "# Test\n",
    "new_kopi = \"Is it love?\"\n",
    "variable_from_sent(new_kopi, english_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXn7WpKGWIzg"
   },
   "source": [
    "## Split into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "34nfNaefWIzh",
    "outputId": "41f4902b-3109-4924-ac76-d316c5501a8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5617, 3)\n",
      "(992, 3)\n",
      "kotak itu kosong .\n",
      "('the box is empty .', 'kotak itu kosong .')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_val = train_test_split(df, test_size=0.15)\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_train.head()\n",
    "\n",
    "indo_tensors = df_train['Indonesian'].apply(lambda s: variable_from_sent(s, indo_vocab))\n",
    "print(df_train.iloc[0]['Indonesian'])\n",
    "df_train\n",
    "\n",
    "english_tensors = df_train['English'].apply(lambda s: variable_from_sent(s, english_vocab))\n",
    "#print(df_train.iloc[0]['English'])\n",
    "#print(english_tensors[0])\n",
    "# Now, each item in `sent_pairs` is our data point.\n",
    "#print(\"############################\")\n",
    "sent_pairs = list(zip(english_tensors.values, indo_tensors.values))\n",
    "#print(sent_pairs[:5])\n",
    "#print(\"############################\")\n",
    "pairs = list(zip(df_train['English'], df_train['Indonesian']))\n",
    "print(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "efvf_EetWIzn",
    "outputId": "54523297-023a-4500-aea9-2fe33458b648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i ll give you a call in the morning .', 'saya akan menelponmu besok pagi .')\n"
     ]
    }
   ],
   "source": [
    "def get_validation_pairs(df_val_in):\n",
    "    indo_val_tensors = df_val_in['Indonesian'].apply(lambda s: variable_from_sent(s, indo_vocab))\n",
    "    english_val_tensors = df_val_in['English'].apply(lambda s: variable_from_sent(s, english_vocab))\n",
    "    val_sent_tensor_pairs = list(zip(english_val_tensors.values, indo_val_tensors.values))\n",
    "    val_sent_pairs = list(zip(df_val_in['English'], df_val_in['Indonesian']))\n",
    "    return val_sent_pairs, val_sent_tensor_pairs\n",
    "\n",
    "\n",
    "val_sent_pairs, val_sent_tensor_pairs = get_validation_pairs(df_val)\n",
    "print(val_sent_pairs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4gcyKGOWIz_"
   },
   "source": [
    "## Define encoder and attention based decoder model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "nkjIwC9vWI0B"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fy8wdwWLWI0F"
   },
   "source": [
    "## Get training and validation set loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "nDRV7_JHWI0H"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_train_loss(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "def get_validation_loss(input_tensor, target_tensor, encoder, decoder, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss = criterion(decoder_output, target_tensor[di])\n",
    "            total_loss += float(loss.item())\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    return total_loss / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePf3q2gFWI0R"
   },
   "source": [
    "## Utilities - required for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9jQVkj64WI0T"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "SAVE_PATH = 'results'\n",
    "\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "  os.makedirs(SAVE_PATH)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xDZRZpqhWI0W"
   },
   "source": [
    "## Training loop and get evaluation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "IJx18gLeWI0Y"
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, batch_size = 512, print_every=1000, save_every=1000, plot_every=100, learning_rate=0.0001):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    val_losses = []\n",
    "    bleu_scores = []\n",
    "\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    #training_pairs = [sent_pairs[i] for i in range(n_iters)]\n",
    "    training_pairs = [random.sample(sent_pairs, batch_size) for i in range(n_iters)]\n",
    "\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    MAX_PATIENCE = 10\n",
    "    patience = MAX_PATIENCE\n",
    "    prev_val_loss =lowest_so_far = prev_bleu =  999\n",
    "    highest_so_far = -np.inf # for bleu\n",
    "    stopping_criteria_on = True\n",
    "    using_bleu_stopping = False\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "\n",
    "        input_tensor = training_pair[0][0]\n",
    "        target_tensor = training_pair[0][1]\n",
    "\n",
    "\n",
    "        loss = get_train_loss(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        stopping_delta = 0.001  # if improvement is not more than this amount after n tries, exit the loop\n",
    "\n",
    "\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('Training loss: %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "            total_val_loss = 0\n",
    "            total_bleu_score = 0\n",
    "            total_val_pairs = len(val_sent_tensor_pairs)\n",
    "\n",
    "            for itr in range(0, len(val_sent_tensor_pairs)):\n",
    "                val_input_tensor = val_sent_tensor_pairs[itr][0]\n",
    "                val_target_tensor = val_sent_tensor_pairs[itr][1]\n",
    "                reference_sent = val_sent_pairs[itr][1]\n",
    "                candidate_sent = translate(val_sent_pairs[itr][0], encoder, decoder)\n",
    "                bleu_score = calculate_bleu_score(reference_sent,candidate_sent)\n",
    "                total_bleu_score += bleu_score\n",
    "                val_loss = get_validation_loss(val_input_tensor, val_target_tensor, encoder, decoder, criterion)\n",
    "                total_val_loss += val_loss\n",
    "\n",
    "            avg_val_loss = total_val_loss / total_val_pairs\n",
    "            val_losses.append(avg_val_loss)\n",
    "            avg_bleu_scores = total_bleu_score / total_val_pairs\n",
    "            bleu_scores.append(avg_bleu_scores)\n",
    "\n",
    "            print('Validation loss: %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                                          iter, iter / n_iters * 100, avg_val_loss))\n",
    "            print('Bleu scores: %s (%d %d%%) %.8f' % (timeSince(start, iter / n_iters),\n",
    "                                                          iter, iter / n_iters * 100, avg_bleu_scores))\n",
    "            if  stopping_criteria_on:\n",
    "                if not using_bleu_stopping:\n",
    "                    if (prev_val_loss - avg_val_loss) > stopping_delta and avg_val_loss < lowest_so_far:\n",
    "                        print(f\"Improvement in validation loss, saving model. Prev {prev_val_loss} Curr {avg_val_loss}\")\n",
    "                        lowest_so_far = avg_val_loss\n",
    "                        encoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_encoder')\n",
    "                        print('save encoder weights to ', encoder_save_path)\n",
    "                        torch.save(encoder.state_dict(), encoder_save_path)\n",
    "                        decoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_decoder')\n",
    "                        print('save decoder weights to ', decoder_save_path)\n",
    "                        torch.save(decoder.state_dict(), decoder_save_path)\n",
    "                        patience = MAX_PATIENCE # reset to max\n",
    "                    else:\n",
    "                        print(f\"No improvement in validation loss, losing patience {patience}\")\n",
    "                        patience -= 1\n",
    "\n",
    "                    if patience == 0:  # break out of training\n",
    "                        break\n",
    "\n",
    "                    prev_val_loss = avg_val_loss\n",
    "                else: # bleu\n",
    "                    if (avg_bleu_scores - prev_bleu) > stopping_delta and avg_bleu_scores > highest_so_far:\n",
    "                        print(f\"Improvement in bleu scores, saving model. Prev {prev_bleu} Curr {avg_bleu_scores}\")\n",
    "                        highest_so_far = avg_bleu_scores\n",
    "                        encoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_encoder')\n",
    "                        print('save encoder weights to ', encoder_save_path)\n",
    "                        torch.save(encoder.state_dict(), encoder_save_path)\n",
    "                        decoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_decoder')\n",
    "                        print('save decoder weights to ', decoder_save_path)\n",
    "                        torch.save(decoder.state_dict(), decoder_save_path)\n",
    "                        patience = MAX_PATIENCE # reset to max\n",
    "                    else:\n",
    "                        print(f\"No improvement in bleu scores, losing patience {patience}\")\n",
    "                        patience -= 1\n",
    "\n",
    "                    if patience == 0:  # break out of training\n",
    "                        break\n",
    "\n",
    "                    prev_bleu = avg_bleu_scores\n",
    "\n",
    "\n",
    "            print(\"##########################################################\")\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "        # save trained encoder and decoder\n",
    "        if iter % save_every == 0:\n",
    "            encoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'encoder', iter)\n",
    "            print('save encoder weights to ', encoder_save_path)\n",
    "            torch.save(encoder.state_dict(), encoder_save_path)\n",
    "            decoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'decoder', iter)\n",
    "            print('save decoder weights to ', decoder_save_path)\n",
    "            torch.save(decoder.state_dict(), decoder_save_path)\n",
    "\n",
    "    showPlot(plot_losses, 'train_plot.png')\n",
    "    showPlot(val_losses, 'validation_plot.png')\n",
    "    showPlot(bleu_scores,'bleu_scores_plot.png')\n",
    "    return plot_losses, val_losses, bleu_scores\n",
    "\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        # input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_tensor = variable_from_sent(sentence, english_vocab)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('</s>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(indo_vocab.id2token[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "\n",
    "def translate(input_sentence, enc, dec):\n",
    "    output_words, attentions = evaluate(\n",
    "        enc, dec, input_sentence)\n",
    "    candidate = ' '.join(output_words)\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uERT7riuWI0b"
   },
   "source": [
    "## Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 8466
    },
    "colab_type": "code",
    "id": "vLkx3FDdWI0c",
    "outputId": "0baaaed0-b3a4-486a-c560-f4373b38fb99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0m 48s (- 60m 4s) (1000 1%) 4.1083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chris\\anaconda3\\envs\\uas-nlp-torch\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\chris\\anaconda3\\envs\\uas-nlp-torch\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\chris\\anaconda3\\envs\\uas-nlp-torch\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 1m 6s (- 81m 43s) (1000 1%) 3.8282\n",
      "Bleu scores: 1m 6s (- 81m 43s) (1000 1%) 0.04078011\n",
      "Improvement in validation loss, saving model. Prev 999 Curr 3.8282055080321866\n",
      "save encoder weights to  results/best_encoder.pth\n",
      "save decoder weights to  results/best_decoder.pth\n",
      "##########################################################\n",
      "save encoder weights to  results/encoder-1000.pth\n",
      "save decoder weights to  results/decoder-1000.pth\n",
      "Training loss: 1m 35s (- 58m 3s) (2000 2%) 3.8020\n",
      "Validation loss: 1m 52s (- 68m 24s) (2000 2%) 3.7255\n",
      "Bleu scores: 1m 52s (- 68m 24s) (2000 2%) 0.04715579\n",
      "Improvement in validation loss, saving model. Prev 3.8282055080321866 Curr 3.7255124863387086\n",
      "save encoder weights to  results/best_encoder.pth\n",
      "save decoder weights to  results/best_decoder.pth\n",
      "##########################################################\n",
      "save encoder weights to  results/encoder-2000.pth\n",
      "save decoder weights to  results/decoder-2000.pth\n",
      "Training loss: 2m 23s (- 57m 18s) (3000 4%) 3.7244\n",
      "Validation loss: 2m 40s (- 64m 20s) (3000 4%) 3.8440\n",
      "Bleu scores: 2m 40s (- 64m 20s) (3000 4%) 0.04841076\n",
      "No improvement in validation loss, losing patience 10\n",
      "##########################################################\n",
      "save encoder weights to  results/encoder-3000.pth\n",
      "save decoder weights to  results/decoder-3000.pth\n",
      "Training loss: 3m 8s (- 55m 52s) (4000 5%) 3.5913\n",
      "Validation loss: 3m 23s (- 60m 11s) (4000 5%) 3.6595\n",
      "Bleu scores: 3m 23s (- 60m 11s) (4000 5%) 0.05391701\n",
      "Improvement in validation loss, saving model. Prev 3.8440319432779715 Curr 3.659497529737715\n",
      "save encoder weights to  results/best_encoder.pth\n",
      "save decoder weights to  results/best_decoder.pth\n",
      "##########################################################\n",
      "save encoder weights to  results/encoder-4000.pth\n",
      "save decoder weights to  results/decoder-4000.pth\n",
      "Training loss: 3m 51s (- 53m 55s) (5000 6%) 3.4641\n",
      "Validation loss: 4m 8s (- 57m 59s) (5000 6%) 3.7836\n",
      "Bleu scores: 4m 8s (- 57m 59s) (5000 6%) 0.05561655\n",
      "No improvement in validation loss, losing patience 10\n",
      "##########################################################\n",
      "save encoder weights to  results/encoder-5000.pth\n",
      "save decoder weights to  results/decoder-5000.pth\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 512\n",
    "encoder1 = EncoderRNN(len(english_vocab), hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, len(indo_vocab), dropout_p=0.5).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=1000)\n",
    "\n",
    "evaluateRandomly(encoder1, attn_decoder1)\n",
    "\n",
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"do you love me?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uc9fOh1SWI0i"
   },
   "source": [
    "## Check some translations - note the below sentences are not there in the training and the validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(translate(\"tom is playing with ball .\", encoder1, attn_decoder1))\n",
    "\n",
    "print(translate(\"she is standing there .\", encoder1, attn_decoder1))\n",
    "\n",
    "print(translate(\"he is a bad man .\", encoder1, attn_decoder1))\n",
    "\n",
    "print(translate(\"he wants to sleep .\", encoder1, attn_decoder1))\n",
    "\n",
    "print(translate(\"i can't see you crying .\", encoder1, attn_decoder1))\n",
    "\n",
    "print(translate(\"my dog is running around .\", encoder1, attn_decoder1))\n",
    "\n",
    "print(translate(\"it is very popular .\", encoder1, attn_decoder1))\n",
    "\n",
    "print(translate(\"she speaks american english to tom's father .\", encoder1, attn_decoder1))\n",
    "\n",
    "print(translate(\"please eat lunch in the afternoon .\", encoder1, attn_decoder1))\n",
    "\n",
    "print(translate(\"i see red roses in the garden .\", encoder1, attn_decoder1))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Eng2Indo Attention Simple corpus - working copy with stopping.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.14 ('uas-nlp-torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "c3adc0486dcd807c46b65bc5c1d74b70fc20571f30c303aa4103f6d2a4f796df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
