{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBart50TokenizerFast,MBartForConditionalGeneration,Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"Helsinki-NLP/opus-100\", \"en-id\")\n",
    "model_mbart = 'facebook/mbart-large-50-one-to-many-mmt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_mbart,src_lang=\"en_XX\",tgt_lang = \"id_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"id\"\n",
    "\n",
    "def preprocess(data):\n",
    "  inputs = [dt[source_lang] for dt in data[\"translation\"]]\n",
    "  targets = [dt[target_lang] for dt in data[\"translation\"]]\n",
    "  model_inputs = tokenizer(inputs, truncation=True)\n",
    "\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(targets, truncation=True)\n",
    "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "  return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(model_mbart)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'learning_rate': 1e-5,\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 10\n",
    "}\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"mbart-large-50-one-to-many-mmt-finetuned-en-to-id\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=hyperparameters['learning_rate'],\n",
    "    per_device_train_batch_size=hyperparameters['batch_size'],\n",
    "    per_device_eval_batch_size=hyperparameters['batch_size'],\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=hyperparameters['num_epochs'],\n",
    "    num_train_epochs=hyperparameters['num_epochs'],\n",
    "    predict_with_generate=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")\n",
    "meteor = evaluate.load('meteor')\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(\n",
    "        decoded_preds, decoded_labels)\n",
    "    result = metric.compute(predictions=decoded_preds,\n",
    "                            references=decoded_labels)\n",
    "    meteor_result = meteor.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels)\n",
    "    prediction_lens = [np.count_nonzero(\n",
    "        pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result = {'bleu': result['score']}\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result[\"meteor\"] = meteor_result[\"meteor\"]\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ecb302a5deb493aa676eb6802247840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc85f94f2fc3492f9d3dbfbf540d6b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1864113807678223, 'eval_bleu': 9.2087, 'eval_gen_len': 14.747, 'eval_meteor': 0.2429, 'eval_runtime': 66.0176, 'eval_samples_per_second': 15.147, 'eval_steps_per_second': 0.485, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7839196eea19490fa69e5471bf1b4a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0488064289093018, 'eval_bleu': 18.451, 'eval_gen_len': 13.771, 'eval_meteor': 0.4071, 'eval_runtime': 990.2109, 'eval_samples_per_second': 1.01, 'eval_steps_per_second': 0.032, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f293533b1c74c8d8930b00f17ace847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9852286577224731, 'eval_bleu': 24.5361, 'eval_gen_len': 13.173, 'eval_meteor': 0.4966, 'eval_runtime': 947.3509, 'eval_samples_per_second': 1.056, 'eval_steps_per_second': 0.034, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39a34bf5bbc4130bc59fcb72f1f8a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9715750217437744, 'eval_bleu': 25.659, 'eval_gen_len': 12.982, 'eval_meteor': 0.5097, 'eval_runtime': 878.4208, 'eval_samples_per_second': 1.138, 'eval_steps_per_second': 0.036, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8271c3677dc641b2bffe247d79652c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9833670854568481, 'eval_bleu': 25.6724, 'eval_gen_len': 12.976, 'eval_meteor': 0.5141, 'eval_runtime': 863.4577, 'eval_samples_per_second': 1.158, 'eval_steps_per_second': 0.037, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e361aa05c84f16b00b1be6ca00a45d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0055649280548096, 'eval_bleu': 25.7811, 'eval_gen_len': 12.749, 'eval_meteor': 0.5102, 'eval_runtime': 442.4429, 'eval_samples_per_second': 2.26, 'eval_steps_per_second': 0.072, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc29d16afee54b24a50ece051ad3a238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.024784803390503, 'eval_bleu': 25.5804, 'eval_gen_len': 12.702, 'eval_meteor': 0.5095, 'eval_runtime': 399.8456, 'eval_samples_per_second': 2.501, 'eval_steps_per_second': 0.08, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23d8d988e7849d9af6ff0d9d8fa586f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0386133193969727, 'eval_bleu': 25.6233, 'eval_gen_len': 12.685, 'eval_meteor': 0.5097, 'eval_runtime': 403.252, 'eval_samples_per_second': 2.48, 'eval_steps_per_second': 0.079, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44b5ccc22c144cabed0f4873a45f5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0506856441497803, 'eval_bleu': 25.5965, 'eval_gen_len': 12.69, 'eval_meteor': 0.5108, 'eval_runtime': 395.0228, 'eval_samples_per_second': 2.531, 'eval_steps_per_second': 0.081, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615387f28e734a778c9ae688d7148f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.054755687713623, 'eval_bleu': 25.4659, 'eval_gen_len': 12.686, 'eval_meteor': 0.5098, 'eval_runtime': 399.8797, 'eval_samples_per_second': 2.501, 'eval_steps_per_second': 0.08, 'epoch': 10.0}\n",
      "{'train_runtime': 9587.6313, 'train_samples_per_second': 1.043, 'train_steps_per_second': 0.033, 'train_loss': 1.6048397064208983, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=320, training_loss=1.6048397064208983, metrics={'train_runtime': 9587.6313, 'train_samples_per_second': 1.043, 'train_steps_per_second': 0.033, 'total_flos': 697985092288512.0, 'train_loss': 1.6048397064208983, 'epoch': 10.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "320/320 [2:39:47<00:00, 7.31s/it]\n",
    "{'eval_loss': 2.1864113807678223, 'eval_bleu': 9.2087, 'eval_gen_len': 14.747, 'eval_meteor': 0.2429, 'eval_runtime': 66.0176, 'eval_samples_per_second': 15.147, 'eval_steps_per_second': 0.485, 'epoch': 1.0}\n",
    "{'eval_loss': 2.0488064289093018, 'eval_bleu': 18.451, 'eval_gen_len': 13.771, 'eval_meteor': 0.4071, 'eval_runtime': 990.2109, 'eval_samples_per_second': 1.01, 'eval_steps_per_second': 0.032, 'epoch': 2.0}\n",
    "{'eval_loss': 1.9852286577224731, 'eval_bleu': 24.5361, 'eval_gen_len': 13.173, 'eval_meteor': 0.4966, 'eval_runtime': 947.3509, 'eval_samples_per_second': 1.056, 'eval_steps_per_second': 0.034, 'epoch': 3.0}\n",
    "{'eval_loss': 1.9715750217437744, 'eval_bleu': 25.659, 'eval_gen_len': 12.982, 'eval_meteor': 0.5097, 'eval_runtime': 878.4208, 'eval_samples_per_second': 1.138, 'eval_steps_per_second': 0.036, 'epoch': 4.0}\n",
    "{'eval_loss': 1.9833670854568481, 'eval_bleu': 25.6724, 'eval_gen_len': 12.976, 'eval_meteor': 0.5141, 'eval_runtime': 863.4577, 'eval_samples_per_second': 1.158, 'eval_steps_per_second': 0.037, 'epoch': 5.0}\n",
    "{'eval_loss': 2.0055649280548096, 'eval_bleu': 25.7811, 'eval_gen_len': 12.749, 'eval_meteor': 0.5102, 'eval_runtime': 442.4429, 'eval_samples_per_second': 2.26, 'eval_steps_per_second': 0.072, 'epoch': 6.0}\n",
    "{'eval_loss': 2.024784803390503, 'eval_bleu': 25.5804, 'eval_gen_len': 12.702, 'eval_meteor': 0.5095, 'eval_runtime': 399.8456, 'eval_samples_per_second': 2.501, 'eval_steps_per_second': 0.08, 'epoch': 7.0}\n",
    "{'eval_loss': 2.0386133193969727, 'eval_bleu': 25.6233, 'eval_gen_len': 12.685, 'eval_meteor': 0.5097, 'eval_runtime': 403.252, 'eval_samples_per_second': 2.48, 'eval_steps_per_second': 0.079, 'epoch': 8.0}\n",
    "{'eval_loss': 2.0506856441497803, 'eval_bleu': 25.5965, 'eval_gen_len': 12.69, 'eval_meteor': 0.5108, 'eval_runtime': 395.0228, 'eval_samples_per_second': 2.531, 'eval_steps_per_second': 0.081, 'epoch': 9.0}\n",
    "{'eval_loss': 2.054755687713623, 'eval_bleu': 25.4659, 'eval_gen_len': 12.686, 'eval_meteor': 0.5098, 'eval_runtime': 399.8797, 'eval_samples_per_second': 2.501, 'eval_steps_per_second': 0.08, 'epoch': 10.0}\n",
    "{'train_runtime': 9587.6313, 'train_samples_per_second': 1.043, 'train_steps_per_second': 0.033, 'train_loss': 1.6048397064208983, 'epoch': 10.0}\n",
    "TrainOutput(global_step=320, training_loss=1.6048397064208983, metrics={'train_runtime': 9587.6313, 'train_samples_per_second': 1.043, 'train_steps_per_second': 0.033, 'total_flos': 697985092288512.0, 'train_loss': 1.6048397064208983, 'epoch': 10.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n"
     ]
    }
   ],
   "source": [
    "# trainer.save_model('opus-mt-en-id-finetuned-en-to-id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aku harap kita semua lulus NLP. Hendrik adalah dosen terbaik di Institut Teknologi Calvin!!!']\n"
     ]
    }
   ],
   "source": [
    "src_text = [\"I hope we all passed NLP. Hendrik is the best lecturer in Calvin Institute of Technology!!!\"]\n",
    "model_path = 'opus-mt-en-id-finetuned-en-to-id'\n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_path,src_lang=\"en_XX\")\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "model_inputs = tokenizer(src_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**model_inputs,forced_bos_token_id=tokenizer.lang_code_to_id[\"id_ID\"], max_new_tokens=360)\n",
    "translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(translation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('uas-nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "317eb77077ba5247a83842478e76361b485ac8e1800fe36a66d95832bc76bd83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
