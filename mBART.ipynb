{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBart50TokenizerFast,MBartForConditionalGeneration,Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"Helsinki-NLP/opus-100\", \"en-id\")\n",
    "model_mbart = 'facebook/mbart-large-50-one-to-many-mmt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_mbart,src_lang=\"en_XX\",tgt_lang = \"id_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"id\"\n",
    "\n",
    "def preprocess(data):\n",
    "  inputs = [dt[source_lang] for dt in data[\"translation\"]]\n",
    "  targets = [dt[target_lang] for dt in data[\"translation\"]]\n",
    "  model_inputs = tokenizer(inputs, truncation=True)\n",
    "\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(targets, truncation=True)\n",
    "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "  return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(model_mbart)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"mbart-large-50-one-to-many-mmt-finetuned-en-to-id\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")\n",
    "meteor = evaluate.load('meteor')\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "   preds = [pred.strip() for pred in preds]\n",
    "   labels = [[label.strip()] for label in labels]\n",
    "   return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "   preds, labels = eval_preds\n",
    "   if isinstance(preds, tuple):\n",
    "       preds = preds[0]\n",
    "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "   # Replace -100 in the labels as we can't decode them.\n",
    "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "   # Some simple post-processing\n",
    "   decoded_preds, decoded_labels = postprocess_text(\n",
    "       decoded_preds, decoded_labels)\n",
    "   result = metric.compute(predictions=decoded_preds,\n",
    "                           references=decoded_labels)\n",
    "   meteor_result = meteor.compute(\n",
    "       predictions=decoded_preds, references=decoded_labels)\n",
    "   prediction_lens = [np.count_nonzero(\n",
    "       pred != tokenizer.pad_token_id) for pred in preds]\n",
    "   result = {'bleu': result['score']}\n",
    "   result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "   result[\"meteor\"] = meteor_result[\"meteor\"]\n",
    "   result = {k: round(v, 4) for k, v in result.items()}\n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m      2\u001b[0m     model,\n\u001b[0;32m      3\u001b[0m     args,\n\u001b[0;32m      4\u001b[0m     train_dataset\u001b[39m=\u001b[39;49msmall_train_dataset,\n\u001b[0;32m      5\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49msmall_eval_dataset,\n\u001b[0;32m      6\u001b[0m     data_collator\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[0;32m      7\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[0;32m      8\u001b[0m     compute_metrics\u001b[39m=\u001b[39;49mcompute_metrics\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\uas-nlp\\lib\\site-packages\\transformers\\trainer_seq2seq.py:57\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     44\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     45\u001b[0m     model: Union[\u001b[39m\"\u001b[39m\u001b[39mPreTrainedModel\u001b[39m\u001b[39m\"\u001b[39m, nn\u001b[39m.\u001b[39mModule] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m     preprocess_logits_for_metrics: Optional[Callable[[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor], torch\u001b[39m.\u001b[39mTensor]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     56\u001b[0m ):\n\u001b[1;32m---> 57\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m     58\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     59\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m     60\u001b[0m         data_collator\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[0;32m     61\u001b[0m         train_dataset\u001b[39m=\u001b[39;49mtrain_dataset,\n\u001b[0;32m     62\u001b[0m         eval_dataset\u001b[39m=\u001b[39;49meval_dataset,\n\u001b[0;32m     63\u001b[0m         tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[0;32m     64\u001b[0m         model_init\u001b[39m=\u001b[39;49mmodel_init,\n\u001b[0;32m     65\u001b[0m         compute_metrics\u001b[39m=\u001b[39;49mcompute_metrics,\n\u001b[0;32m     66\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m     67\u001b[0m         optimizers\u001b[39m=\u001b[39;49moptimizers,\n\u001b[0;32m     68\u001b[0m         preprocess_logits_for_metrics\u001b[39m=\u001b[39;49mpreprocess_logits_for_metrics,\n\u001b[0;32m     69\u001b[0m     )\n\u001b[0;32m     71\u001b[0m     \u001b[39m# Override self.model.generation_config if a GenerationConfig is specified in args.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[39m# Priority: args.generation_config > model.generation_config > default GenerationConfig.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgeneration_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\uas-nlp\\lib\\site-packages\\transformers\\trainer.py:383\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m args\n\u001b[0;32m    382\u001b[0m \u001b[39m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[1;32m--> 383\u001b[0m enable_full_determinism(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mseed) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mfull_determinism \u001b[39melse\u001b[39;00m set_seed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mseed)\n\u001b[0;32m    384\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhp_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\uas-nlp\\lib\\site-packages\\transformers\\trainer_utils.py:99\u001b[0m, in \u001b[0;36mset_seed\u001b[1;34m(seed, deterministic)\u001b[0m\n\u001b[0;32m     97\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(seed)\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m is_torch_available():\n\u001b[1;32m---> 99\u001b[0m     torch\u001b[39m.\u001b[39;49mmanual_seed(seed)\n\u001b[0;32m    100\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mmanual_seed_all(seed)\n\u001b[0;32m    101\u001b[0m     \u001b[39m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\uas-nlp\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m prior \u001b[39m=\u001b[39m set_eval_frame(callback)\n\u001b[0;32m    450\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    452\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\uas-nlp\\lib\\site-packages\\torch\\_dynamo\\external_utils.py:36\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[0;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 36\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\uas-nlp\\lib\\site-packages\\torch\\random.py:40\u001b[0m, in \u001b[0;36mmanual_seed\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcuda\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_is_in_bad_fork():\n\u001b[1;32m---> 40\u001b[0m     torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mmanual_seed_all(seed)\n\u001b[0;32m     42\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmps\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mmps\u001b[39m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\uas-nlp\\lib\\site-packages\\torch\\cuda\\random.py:126\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m    123\u001b[0m         default_generator \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdefault_generators[i]\n\u001b[0;32m    124\u001b[0m         default_generator\u001b[39m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m--> 126\u001b[0m _lazy_call(cb, seed_all\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\uas-nlp\\lib\\site-packages\\torch\\cuda\\__init__.py:223\u001b[0m, in \u001b[0;36m_lazy_call\u001b[1;34m(callable, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_lazy_call\u001b[39m(\u001b[39mcallable\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    222\u001b[0m     \u001b[39mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 223\u001b[0m         \u001b[39mcallable\u001b[39;49m()\n\u001b[0;32m    224\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m         \u001b[39m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[0;32m    226\u001b[0m         \u001b[39m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         \u001b[39m# else here if this ends up being important.\u001b[39;00m\n\u001b[0;32m    228\u001b[0m         \u001b[39mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[1;32mc:\\Users\\chris\\anaconda3\\envs\\uas-nlp\\lib\\site-packages\\torch\\cuda\\random.py:124\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[1;34m()\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(device_count()):\n\u001b[0;32m    123\u001b[0m     default_generator \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdefault_generators[i]\n\u001b[1;32m--> 124\u001b[0m     default_generator\u001b[39m.\u001b[39;49mmanual_seed(seed)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('opus-mt-en-id-finetuned-en-to-id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = [\"I hope we all passed NLP. Hendrik is the best lecturer in Calvin Institute of Technology!!!\"]\n",
    "model_path = 'model\\mbart-large-50-one-to-many-mmt-finetuned-en-to-id'\n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_path,src_lang=\"en_XX\")\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "model_inputs = tokenizer(src_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**model_inputs,forced_bos_token_id=tokenizer.lang_code_to_id[\"id_ID\"], max_new_tokens=360)\n",
    "translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(translation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('uas-nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "317eb77077ba5247a83842478e76361b485ac8e1800fe36a66d95832bc76bd83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
