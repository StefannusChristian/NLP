{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Kita akan membangun detektor dan model- model yang taat, untuk keduanya, kita akan mulai dengan model prelatih, maka kita akan membekukan beban di belakang tulang semua lapisan kecuali lapisan kelas akhir.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "src_text = \"translate to Indonesia: \" + \"Here we build our detector and recognizer models. For both, weâ€™ll start with pretrained models. Note that for the recognizer, we freeze the weights in the backbone all the layers except for the final classification layer.\"\n",
    "model_path = \"D:\\\\Users\\\\IBDA\\\\Documents\\\\TA Jacob\\\\T5_checkpoints\\\\flan-t5-2\\\\checkpoint-218500\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# model_path = 'model/flan-t5-finetuned-en-to-id'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True), max_new_tokens=512)\n",
    "[tokenizer.decode(t, skip_special_tokens=True) for t in translated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['di sini kita membangun detektor dan model pengenal kita untuk keduanya, kita akan mulai dengan model yang dilatih perhatikan bahwa untuk penerima, kita membekukan berat di tulang belakang (semua lapisan kecuali lapisan klasifikasi akhir).']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "src_text = \">>ind<< Here we build our detector and recognizer models. For both, weâ€™ll start with pretrained models. Note that for the recognizer, we freeze the weights in the backbone (all the layers except for the final classification layer).\"\n",
    "model_path = 'D:\\\\Users\\\\IBDA\\\\Documents\\\\TA Jacob\\\\model\\\\opus-mt-en-id-4'\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
    "model = MarianMTModel.from_pretrained(model_path)\n",
    "\n",
    "translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n",
    "[tokenizer.decode(t, skip_special_tokens=True) for t in translated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['di sini kita membangun model pendeteksi dan pengenal kami. untuk keduanya, kita akan mulai dengan model terhembus. perhatikan bahwa untuk pengenal, kita membekukan berat di tulang punggung (semua lapisan kecuali untuk lapisan klasifikasi final).']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
    "\n",
    "src_text = \"Here we build our detector and recognizer models. For both, weâ€™ll start with pretrained models. Note that for the recognizer, we freeze the weights in the backbone (all the layers except for the final classification layer).\"\n",
    "model_path = 'model\\mbart-large-50-5'\n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_path,src_lang=\"en_XX\")\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "model_inputs = tokenizer(src_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**model_inputs,forced_bos_token_id=tokenizer.lang_code_to_id[\"id_ID\"], max_new_tokens=360)\n",
    "translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "translation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta-jacob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
